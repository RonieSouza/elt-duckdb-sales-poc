<p align="center">
  <img src="https://upload.wikimedia.org/wikipedia/commons/4/40/DuckDB_logo.svg" alt="DuckDB Logo" width="150"/>
</p>
<p align="center">
  <b>Este projeto utiliza <a href="https://duckdb.org">DuckDB</a> motor principal de processamento analÃ­tico.</b><br/>
</p>

# ELT-DUCKDB-SALES-POC

PoC de ELT com Python e DuckDB em Docker, estruturada em camadas **Bronze**, **Silver** e **Gold**, com modelagem final em **Star Schema (Kimball)**. Foco em leveza, performance, simplicidade aproveitando recursos open source.

---

## ğŸš€ Sobre o Projeto

Este projeto Ã© uma **prova de conceito (PoC)** desenvolvida para demonstrar um processo completo de **ELT local** com foco em:

- Estrutura por camadas: **Bronze**, **Silver**, **Gold**.
- Leveza e desempenho com **DuckDB**.
- Modelagem dimensional (Kimball) na camada Gold.
- Armazenamento local com **arquivos .parquet**.
- ExecuÃ§Ã£o rÃ¡pida e portÃ¡til com **Docker**.
- Desenvolvimento orientado por **notebooks** e scripts Python.

Utiliza um dataset de vendas do [Kaggle](https://www.kaggle.com/datasets/vinothkannaece/sales-dataset) em um ambiente isolado e controlado para fins de estudo.

---

## ğŸ§± Camadas de Dados

- **Source**: dados brutos (.csv).
- **Bronze**: cÃ³pia fiel da fonte (`raw`), sem alteraÃ§Ãµes.
- **Silver**: dados limpos e normalizados (sem duplicidades).
- **Gold**: modelagem em **Star Schema**, visando consumo analÃ­tico.

---

## ğŸ§¬ Modelagem Dimensional

A partir do arquivo `sales.csv`, o projeto gera na camada **Gold**:

- `dim_customer_type.parquet`
- `dim_date.parquet`
- `dim_payment_method.parquet`
- `dim_product.parquet`
- `dim_region.parquet`
- `dim_representative.parquet`
- `dim_sales_channel.parquet`
- `fact_sales.parquet`

---

## ğŸ¥ Por que DuckDB?

O **DuckDB** Ã© um banco de dados analÃ­tico **leve e embutido**, ideal para processamento local e Ã¡gil de dados:

- Armazenamento em colunas.
- IntegraÃ§Ã£o nativa com CSV e Parquet.
- Suporte a transaÃ§Ãµes ACID.
- AltÃ­ssimo desempenho local.
- NÃ£o precisa de servidor.

Ideal para cenÃ¡rios menores ou mÃ©dios que nÃ£o justificam ferramentas pesadas como Spark â€” que, embora excelentes, devem ser reservadas para contextos de **Big Data** (>500GB ou TBs de dados).

---

## ğŸ§ª Estrutura do Projeto

```bash
elt-duckdb-sales-poc/
â”‚
â”œâ”€â”€ data/                     # Dados locais (simula datalake)
â”‚   â”œâ”€â”€ source/               # CSV original
â”‚   â”œâ”€â”€ bronze/               # Raw data (copiado)
â”‚   â”œâ”€â”€ silver/               # Dados limpos
â”‚   â””â”€â”€ gold/                 # Modelagem dimensional (star schema)
â”‚
â”œâ”€â”€ notebooks/               # Desenvolvimento em notebooks
â”‚   â”œâ”€â”€ bronze_to_silver.ipynb
â”‚   â””â”€â”€ silver_to_gold.ipynb
â”‚
â”œâ”€â”€ scripts/                 # Scripts .py convertidos dos notebooks
â”‚   â”œâ”€â”€ source_to_bronze.py
â”‚   â”œâ”€â”€ bronze_to_silver.py
â”‚   â””â”€â”€ silver_to_gold.py
â”‚
â”œâ”€â”€ venv/                    # Ambiente virtual local
â”‚
â”œâ”€â”€ run_scripts.py           # OrquestraÃ§Ã£o local simples dos scripts
â”œâ”€â”€ convert_notebooks.py     # Conversor de notebooks para scripts
â”œâ”€â”€ requirements.txt         # DependÃªncias para desenvolvimento
â”œâ”€â”€ Dockerfile               # Imagem Docker leve (~268MB)
â””â”€â”€ .gitignore
```

---

## ğŸ³ Docker

A imagem Docker Ã© extremamente leve (~268MB) e contÃ©m apenas o essencial:
```bash
FROM python:3.13-slim

ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1

RUN pip install --no-cache-dir --upgrade pip duckdb

WORKDIR /app

CMD ["python", "run_scripts.py"]
```

---

## ğŸ“¦ InstalaÃ§Ã£o

Clone o repositÃ³rio e instale as dependÃªncias para ambiente local de desenvolvimento:
```bash
git clone https://github.com/seu-usuario/elt-duckdb-sales-poc.git
cd elt-duckdb-sales-poc
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

---

## ğŸ› ï¸ PÃ³s-clonagem (opcional)

ApÃ³s clonar o repositÃ³rio, Ã© possÃ­vel converter os notebooks (`.ipynb`) para scripts Python (`.py`) utilizando o script auxiliar incluso no projeto. Embora os arquivos `.py` jÃ¡ estejam prontos no repositÃ³rio, este passo demonstra como a conversÃ£o pode ser feita facilmente:

```bash
python convert_notebooks.py
```

---

## ğŸ³ ExecuÃ§Ã£o com Docker

Crie a imagem:
```bash
docker build -t elt-duckdb-sales .
```
Rode o container:
```bash
docker run --rm -v .:/app elt-duckdb-sales
```

---

## ğŸ§  ObservaÃ§Ãµes

 - O requirements.txt contÃ©m dependÃªncias voltadas ao desenvolvimento, especialmente para uso em Jupyter Notebooks.
 - O ambiente em Docker Ã© minimalista, garantindo portabilidade e performance.
 - Foco na simulaÃ§Ã£o de um Data Lake local, gerando arquivos .parquet como produto final.

---

## âœ… ConsideraÃ§Ãµes Finais

Esse projeto demonstra que Ã© possÃ­vel realizar pipelines ELT robustos, performÃ¡ticos e bem modelados sem recorrer a ferramentas pesadas como Spark em todos os cenÃ¡rios.
Simplicidade, leveza e boas prÃ¡ticas ainda sÃ£o viÃ¡veis â€” e, em muitos casos, mais adequadas.

---

## ğŸ“„ LicenÃ§a

MIT
